---
title: "제8장: 탐욕 알고리즘 (Greedy Algorithms)"
excerpt: "매 순간 최적이라고 생각되는 선택을 통해 전체의 최적해를 찾아가는 탐욕 알고리즘의 원리와 배낭 문제, 허프만 코딩, MST, 다익스트라 등 주요 응용 사례를 심도 있게 다룹니다."
tags: [algorithm, greedy, python, optimization, graph theory]
header:
  teaser: https://drive.google.com/thumbnail?id=1lkXN9-sYkNjHiBGc5cl_-pEltAkTfSZJ&sz=w1000
---

이 문서는 **탐욕 알고리즘(Greedy Algorithm)**의 개념과 이를 활용한 다양한 최적화 문제를 다룹니다. 탐욕 알고리즘은 문제를 해결하는 과정에서 매 순간, 그 단계에서 가장 최선이라고 여겨지는 선택(Local Optimum)을 함으로써 전체적인 최적해(Global Optimum)에 도달하려는 방식입니다. 동적 계획법(DP)보다 빠르지만 모든 문제에 대해 최적해를 보장하지는 않으므로, 이 기법이 적용 가능한 문제 유형을 파악하는 것이 중요합니다.

## 8.1 탐욕 알고리즘 개요

탐욕 알고리즘은 최적화 문제를 해결하는 데 주로 사용됩니다. 문제 해결 과정을 여러 단계로 나누고, 각 단계에서 다음과 같은 절차를 따릅니다 .

1.  **선택 (Selection):** 현재 상태에서 가장 이익이 되는(최적인) 항목을 선택합니다.
2.  **타당성 검사 (Feasibility):** 선택한 항목이 문제의 제약 조건을 위반하지 않는지 확인합니다.
3.  **해결 (Solution):** 타당하다면 해당 항목을 해(Solution)에 포함시킵니다.

### 8.1.1 탐욕 알고리즘 vs 동적 계획법
* **탐욕 알고리즘 (Greedy):** 각 단계에서 지역적 최적을 선택하며, 한 번 선택한 결정은 번복하지 않습니다. 빠르고 효율적이지만, 항상 전역 최적해를 보장하지는 않습니다. (예: 분수 배낭 문제, MST)
* **동적 계획법 (DP):** 모든 가능한 하위 문제의 해를 고려하여 최적해를 찾습니다. 항상 정확한 답을 보장하지만, 계산 비용이 큽니다. (예: 0/1 배낭 문제)

## 8.2 분수 배낭 문제 (Fractional Knapsack Problem)

0/1 배낭 문제와 달리, 물건을 쪼개서 담을 수 있는 경우입니다. 이 경우 탐욕적 접근이 완벽하게 작동합니다. 전략은 **무게당 가치(Profit/Weight)**가 높은 순서대로 물건을 담는 것입니다.

* **입력:** 물건의 개수 $n$, 배낭 용량 $m$, 각 물건의 이익 $P$와 무게 $W$ 
* **전략:** $P_i / W_i$ 비율이 높은 순서로 정렬하여 배낭을 채웁니다. 공간이 부족하면 마지막 물건을 쪼개서 채웁니다.

[그림: 0/1 배낭 문제와 분수 배낭 문제의 차이점 비교]

```python
# 분수 배낭 문제 (Fractional Knapsack) 구현
class Item:
    def __init__(self, profit, weight):
        self.profit = profit
        self.weight = weight

def fractional_knapsack(capacity, items):
    # 무게당 가치(profit/weight)를 기준으로 내림차순 정렬
    items.sort(key=lambda x: x.profit / x.weight, reverse=True)
    
    total_value = 0.0
    
    for item in items:
        if capacity == 0:
            break
            
        if item.weight <= capacity:
            # 물건을 통째로 넣음
            capacity -= item.weight
            total_value += item.profit
        else:
            # 배낭 용량이 부족하면 물건을 쪼개서 남은 만큼만 넣음
            total_value += item.profit * (capacity / item.weight)
            capacity = 0
            
    return total_value

# 실행 예제 
items = [Item(1, 2), Item(2, 3), Item(5, 4), Item(6, 5)]
capacity = 8
print(f"최대 이익: {fractional_knapsack(capacity, items)}")
````

## 8.3 작업 스케줄링 (Job Sequencing with Deadlines)

각 작업(Task)은 마감 시간(Deadline)과 이익(Profit)을 가집니다. 모든 작업은 수행하는 데 1단위 시간이 걸립니다. 마감 시간 내에 완료하여 얻을 수 있는 이익을 최대화하는 문제입니다.

  * **전략:** 이익이 높은 순서대로 작업을 정렬합니다. 가장 이익이 높은 작업부터, 가능한 마감 시간 내의 **가장 늦은 빈 시간 슬롯**에 배정합니다 .

<!-- end list -->

```python
# 작업 스케줄링 구현
def job_scheduling(jobs, t):
    # jobs: [Job ID, Deadline, Profit]
    # 이익(Profit) 기준으로 내림차순 정렬
    jobs.sort(key=lambda x: x[2], reverse=True)
    
    n = len(jobs)
    # 결과 저장 배열 (False로 초기화)
    result = [False] * t
    # 작업 순서 저장
    job_sequence = ['-1'] * t
    
    total_profit = 0
    
    for i in range(n):
        # 마감 시간(jobs[i][1])부터 역순으로 빈 슬롯 탐색
        # 인덱스는 0부터 시작하므로 min(t-1, deadline-1)
        for j in range(min(t - 1, jobs[i][1] - 1), -1, -1):
            if result[j] is False:
                result[j] = True
                job_sequence[j] = jobs[i][0]
                total_profit += jobs[i][2]
                break
                
    return job_sequence, total_profit

# 실행 예제
# Job ID, Deadline, Profit
jobs = [['T1', 2, 20], ['T2', 2, 15], ['T3', 1, 10], ['T4', 3, 5], ['T5', 3, 1]]
time_slots = 3
seq, profit = job_scheduling(jobs, time_slots)
print(f"작업 순서: {seq}, 총 이익: {profit}")
```

## 8.4 허프만 코딩 (Huffman Coding)

데이터 압축에 사용되는 알고리즘으로, 문자의 빈도수에 따라 가변 길이 코드(Variable-Length Code)를 부여합니다. 빈도수가 높은 문자에는 짧은 코드를, 낮은 문자에는 긴 코드를 할당하여 전체 비트 수를 줄입니다.

  * **최적 병합 패턴 (Optimal Merge Pattern):** 가장 빈도수가 낮은 두 노드를 합쳐 새로운 노드를 만드는 과정을 반복하여 이진 트리를 구성합니다. 이는 탐욕적인 접근 방식입니다.

[그림: 허프만 트리 구성 과정 및 0/1 비트 할당]

```python
# 허프만 코딩 구현
import heapq

class Node:
    def __init__(self, char, freq):
        self.char = char
        self.freq = freq
        self.left = None
        self.right = None
        
    # 힙 정렬을 위한 비교 연산자
    def __lt__(self, other):
        return self.freq < other.freq

def huffman_coding(char_freq):
    # 최소 힙 생성
    heap = [Node(char, freq) for char, freq in char_freq.items()]
    heapq.heapify(heap)
    
    # 힙에 노드가 하나 남을 때까지 반복 (트리 생성)
    while len(heap) > 1:
        left = heapq.heappop(heap) # 가장 빈도 낮은 노드 1
        right = heapq.heappop(heap) # 가장 빈도 낮은 노드 2
        
        # 두 노드를 합친 새로운 노드 생성 (문자는 없음)
        merged = Node(None, left.freq + right.freq)
        merged.left = left
        merged.right = right
        
        heapq.heappush(heap, merged)
        
    return heap[0]

def print_codes(node, code=""):
    if node is None:
        return
    if node.char is not None:
        print(f"{node.char}: {code}")
    # 왼쪽은 0, 오른쪽은 1 할당
    print_codes(node.left, code + "0")
    print_codes(node.right, code + "1")

# 실행 예제
freq = {'A': 3, 'B': 5, 'C': 6, 'D': 4, 'E': 2}
root = huffman_coding(freq)
print("Huffman Codes:")
print_codes(root)
```

## 8.5 최소 비용 신장 트리 (MST, Minimum Spanning Tree)

그래프의 모든 정점을 연결하면서 간선의 가중치 합이 최소가 되는 트리(사이클이 없는 부분 그래프)를 찾는 문제입니다. $V$개의 정점이 있다면 MST는 반드시 $V-1$개의 간선을 가집니다.

### 8.5.1 프림 알고리즘 (Prim's Algorithm)

시작 정점에서 출발하여, 현재 트리에 연결된 간선 중 가장 가중치가 작은 간선을 선택하여 트리를 확장해 나가는 방식입니다 .

[그림: 프림 알고리즘의 단계별 정점 선택 과정]

```python
# 프림 알고리즘 구현
import heapq

def prim_mst(graph, start_node):
    mst_cost = 0
    visited = set()
    # (weight, node) - 우선순위 큐
    min_heap = [(0, start_node)]
    
    while min_heap:
        weight, u = heapq.heappop(min_heap)
        
        if u in visited:
            continue
            
        visited.add(u)
        mst_cost += weight
        
        for v, w in graph[u]:
            if v not in visited:
                heapq.heappush(min_heap, (w, v))
                
    return mst_cost

# 실행 예제 (인접 리스트)
graph = {
    1: [(6, 10), (2, 28)],
    2: [(1, 28), (7, 14), (3, 16)],
    3: [(2, 16), (4, 12)],
    4: [(3, 12), (7, 18), (5, 22)],
    5: [(4, 22), (6, 25), (7, 24)],
    6: [(1, 10), (5, 25)],
    7: [(2, 14), (5, 24), (4, 18)]
}
print(f"MST Cost (Prim's): {prim_mst(graph, 1)}")
```

### 8.5.2 크루스칼 알고리즘 (Kruskal's Algorithm)

간선들을 가중치 순으로 오름차순 정렬한 뒤, 사이클을 형성하지 않는 간선을 순서대로 선택합니다 . 사이클 판별을 위해 유니온-파인드(Union-Find) 자료구조를 사용합니다.

[그림: 크루스칼 알고리즘의 간선 선택 및 사이클 방지]

```python
# 크루스칼 알고리즘 구현
class UnionFind:
    def __init__(self, n):
        self.parent = list(range(n + 1))
        
    def find(self, i):
        if self.parent[i] == i:
            return i
        return self.find(self.parent[i])
        
    def union(self, i, j):
        root_i = self.find(i)
        root_j = self.find(j)
        if root_i != root_j:
            self.parent[root_i] = root_j
            return True
        return False

def kruskal_mst(n, edges):
    # edges: (weight, u, v)
    edges.sort() # 가중치 기준 정렬
    uf = UnionFind(n)
    mst_cost = 0
    edges_count = 0
    
    for w, u, v in edges:
        if uf.union(u, v): # 사이클이 형성되지 않으면 선택
            mst_cost += w
            edges_count += 1
            if edges_count == n - 1: # V-1개 간선 선택 시 종료
                break
                
    return mst_cost

# 실행 예제
# (weight, u, v)
edges = [
    (10, 1, 6), (28, 1, 2), (14, 2, 7), (16, 2, 3),
    (12, 3, 4), (22, 4, 5), (18, 4, 7), (25, 5, 6), (24, 5, 7)
]
n_vertices = 7
print(f"MST Cost (Kruskal's): {kruskal_mst(n_vertices, edges)}")
```

## 8.6 다익스트라 알고리즘 (Dijkstra's Algorithm)

단일 시작점에서 다른 모든 정점까지의 최단 경로를 찾는 알고리즘입니다.

  * **원리:** 시작점의 거리를 0, 나머지는 무한대로 초기화합니다. 방문하지 않은 노드 중 최단 거리가 가장 짧은 노드를 선택하고, 그 노드를 거쳐가는 경로가 기존 경로보다 짧으면 거리를 갱신(Relaxation)합니다.
  * **완화(Relaxation) 조건:** `if d[u] + c(u, v) < d[v]: d[v] = d[u] + c(u, v)` 
  * **한계:** 음수 가중치를 가진 간선이 있을 경우 제대로 작동하지 않습니다.

[그림: 다익스트라 알고리즘의 완화(Relaxation) 과정]

```python
# 다익스트라 알고리즘 구현
import heapq

def dijkstra(graph, start):
    # 거리 테이블 초기화 (무한대)
    distances = {node: float('inf') for node in graph}
    distances[start] = 0
    pq = [(0, start)]
    
    while pq:
        current_dist, current_node = heapq.heappop(pq)
        
        # 이미 처리된 거리보다 긴 경우 스킵
        if current_dist > distances[current_node]:
            continue
            
        for neighbor, weight in graph[current_node]:
            distance = current_dist + weight
            
            # 완화 (Relaxation)
            if distance < distances[neighbor]:
                distances[neighbor] = distance
                heapq.heappush(pq, (distance, neighbor))
                
    return distances

# 실행 예제 (인접 리스트)
graph_dijkstra = {
    1: [(2, 2), (3, 4)],
    2: [(4, 7), (3, 1)],
    3: [(5, 3)],
    4: [(6, 2)],
    5: [(4, 2), (6, 5)],
    6: []
}
# 주의: 예제 그래프 구조는 PDF의 그림 46페이지를 기반으로 단순화함
print(f"최단 거리: {dijkstra(graph_dijkstra, 1)}")
```

## 8.7 결론

탐욕 알고리즘은 직관적이고 구현하기 쉬우며, 많은 경우에 효율적인 근사해 또는 최적해를 제공합니다.

  * **성공 사례:** 분수 배낭 문제, 허프만 코딩, MST(프림, 크루스칼), 다익스트라(음수 간선 없을 때) .
  * **실패 사례:** 0/1 배낭 문제, 음수 간선이 있는 최단 경로 문제.

문제가 **탐욕적 선택 속성(Greedy Choice Property)**과 **최적 부분 구조(Optimal Substructure)**를 가질 때 탐욕 알고리즘을 사용하여 최적해를 구할 수 있습니다.
